import os
import sys
import psycopg2
import psycopg2.extras
import time
import asyncio
import concurrent.futures
from datetime import datetime, timedelta
from flask import Flask, jsonify, request
from flask_cors import CORS
import json
import threading
import schedule
from collections import defaultdict
import hashlib

# DON'T CHANGE THIS !!!
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

app = Flask(__name__)
CORS(app, origins="*")

# Configura√ß√µes
app.config['SECRET_KEY'] = 'real_data_service_v2_secret_2025'
app.config['MAX_WORKERS'] = 4  # Para processamento paralelo

# Configura√ß√µes do banco externo (opera√ß√£o)
EXTERNAL_DB_CONFIG = {
    'host': '177.115.223.216',
    'port': 5999,
    'database': 'dados_interno',
    'user': 'userschapz',
    'password': 'mschaphz8881!',
    'connect_timeout': 30
}

# Cache adaptado para tabelas reais
data_cache = {
    'users': defaultdict(list),
    'transactions': defaultdict(list),
    'affiliates': defaultdict(list),
    'bets': defaultdict(list),
    'metadata': {
        'last_sync': None,
        'sync_status': 'never_synced',
        'total_records': 0,
        'partitions': 0,
        'sync_duration': 0
    }
}

# Pool de conex√µes
connection_pool = []
pool_lock = threading.Lock()

# Lock para thread safety
cache_lock = threading.Lock()

def create_connection_pool(size=5):
    """Cria pool de conex√µes para melhor performance"""
    global connection_pool
    with pool_lock:
        for _ in range(size):
            try:
                conn = psycopg2.connect(**EXTERNAL_DB_CONFIG)
                connection_pool.append(conn)
            except Exception as e:
                print(f"Erro ao criar conex√£o no pool: {e}")

def get_pooled_connection():
    """Obt√©m conex√£o do pool"""
    with pool_lock:
        if connection_pool:
            return connection_pool.pop()
    
    # Se n√£o h√° conex√µes no pool, cria uma nova
    try:
        return psycopg2.connect(**EXTERNAL_DB_CONFIG)
    except Exception as e:
        print(f"Erro ao criar nova conex√£o: {e}")
        return None

def return_connection(conn):
    """Retorna conex√£o para o pool"""
    if conn and not conn.closed:
        with pool_lock:
            if len(connection_pool) < 10:  # Limite do pool
                connection_pool.append(conn)
            else:
                conn.close()

def fetch_data_partition(table_name, offset, limit):
    """Busca uma parti√ß√£o de dados de forma paralela"""
    conn = get_pooled_connection()
    if not conn:
        return []
    
    try:
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        
        # Queries adaptadas para as tabelas reais do banco
        queries = {
            'users': f"""
                SELECT user_id as id, user_id as nome, 
                       CONCAT('user_', user_id) as email,
                       'user' as tipo_usuario,
                       register_date as data_cadastro,
                       EXTRACT(EPOCH FROM register_date) as timestamp_cadastro,
                       'ativo' as status
                FROM cadastro 
                ORDER BY user_id
                OFFSET {offset} LIMIT {limit}
            """,
            'transactions': f"""
                SELECT 
                    CONCAT('dep_', id) as id,
                    user_id as usuario_id,
                    amount as valor,
                    'deposito' as tipo_transacao,
                    status,
                    'Dep√≥sito' as descricao,
                    data_deposito as data_transacao,
                    EXTRACT(EPOCH FROM data_deposito) as timestamp_transacao
                FROM depositos 
                WHERE data_deposito >= NOW() - INTERVAL '60 days'
                
                UNION ALL
                
                SELECT 
                    CONCAT('saq_', id) as id,
                    user_id as usuario_id,
                    valor,
                    'saque' as tipo_transacao,
                    status,
                    'Saque' as descricao,
                    data_saques as data_transacao,
                    EXTRACT(EPOCH FROM data_saques) as timestamp_transacao
                FROM saques 
                WHERE data_saques >= NOW() - INTERVAL '60 days'
                
                ORDER BY data_transacao DESC
                OFFSET {offset} LIMIT {limit}
            """,
            'affiliates': f"""
                SELECT 
                    id,
                    user_afil as afiliado_id,
                    user_id as usuario_indicado_id,
                    tracked_type_id as tipo_vinculo,
                    'ativo' as status,
                    CURRENT_TIMESTAMP as data_ativacao,
                    EXTRACT(EPOCH FROM CURRENT_TIMESTAMP) as timestamp_ativacao
                FROM tracked 
                WHERE tracked_type_id = 1
                ORDER BY user_afil
                OFFSET {offset} LIMIT {limit}
            """,
            'bets': f"""
                SELECT 
                    casino_id as id,
                    user_id,
                    game_name,
                    bet_amount as valor_aposta,
                    earned_value as valor_ganho,
                    status,
                    played_date as data_aposta,
                    EXTRACT(EPOCH FROM played_date) as timestamp_aposta
                FROM casino_bets_v 
                WHERE played_date >= NOW() - INTERVAL '30 days'
                ORDER BY played_date DESC
                OFFSET {offset} LIMIT {limit}
            """
        }
        
        if table_name in queries:
            cursor.execute(queries[table_name])
            results = cursor.fetchall()
            return [dict(row) for row in results]
        else:
            return []
            
    except Exception as e:
        print(f"Erro ao buscar parti√ß√£o {offset}-{offset+limit} da tabela {table_name}: {e}")
        return []
    finally:
        return_connection(conn)

def sync_table_parallel(table_name, max_workers=4):
    """Sincroniza uma tabela usando processamento paralelo"""
    partition_size = 1000
    all_data = []
    
    # Primeiro, descobrir o total de registros
    conn = get_pooled_connection()
    if not conn:
        return []
    
    try:
        cursor = conn.cursor()
        
        count_queries = {
            'users': "SELECT COUNT(*) FROM cadastro",
            'transactions': """
                SELECT (
                    (SELECT COUNT(*) FROM depositos WHERE data_deposito >= NOW() - INTERVAL '60 days') +
                    (SELECT COUNT(*) FROM saques WHERE data_saques >= NOW() - INTERVAL '60 days')
                ) as total
            """,
            'affiliates': "SELECT COUNT(*) FROM tracked WHERE tracked_type_id = 1",
            'bets': "SELECT COUNT(*) FROM casino_bets_v WHERE played_date >= NOW() - INTERVAL '30 days'"
        }
        
        if table_name in count_queries:
            cursor.execute(count_queries[table_name])
            total_records = cursor.fetchone()[0]
        else:
            return []
            
    except Exception as e:
        print(f"Erro ao contar registros de {table_name}: {e}")
        return []
    finally:
        return_connection(conn)
    
    if total_records == 0:
        return []
    
    # Calcular parti√ß√µes
    partitions = [(i, min(partition_size, total_records - i)) 
                  for i in range(0, total_records, partition_size)]
    
    print(f"Sincronizando {table_name}: {total_records} registros em {len(partitions)} parti√ß√µes")
    
    # Processar parti√ß√µes em paralelo
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(fetch_data_partition, table_name, offset, limit)
            for offset, limit in partitions
        ]
        
        for future in concurrent.futures.as_completed(futures):
            try:
                partition_data = future.result()
                all_data.extend(partition_data)
            except Exception as e:
                print(f"Erro ao processar parti√ß√£o de {table_name}: {e}")
    
    return all_data

def sync_all_data_v2():
    """Sincroniza√ß√£o otimizada com processamento paralelo"""
    start_time = time.time()
    
    with cache_lock:
        try:
            data_cache['metadata']['sync_status'] = 'syncing'
            
            # Sincronizar tabelas reais do banco
            tables = ['users', 'transactions', 'affiliates', 'bets']
            total_records = 0
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                # Executar sincroniza√ß√£o de tabelas em paralelo
                futures = {
                    executor.submit(sync_table_parallel, table): table 
                    for table in tables
                }
                
                for future in concurrent.futures.as_completed(futures):
                    table = futures[future]
                    try:
                        data = future.result()
                        
                        # Particionar dados por hash para distribui√ß√£o
                        partitioned_data = defaultdict(list)
                        for record in data:
                            partition_key = hash(str(record.get('id', 0))) % 4
                            partitioned_data[partition_key].append(record)
                        
                        data_cache[table] = partitioned_data
                        total_records += len(data)
                        
                        print(f"Sincronizados {len(data)} registros de {table} em {len(partitioned_data)} parti√ß√µes")
                        
                    except Exception as e:
                        print(f"Erro ao sincronizar {table}: {e}")
            
            # Atualizar metadata
            sync_duration = time.time() - start_time
            data_cache['metadata'].update({
                'last_sync': datetime.now(),
                'sync_status': 'success',
                'total_records': total_records,
                'partitions': 4,
                'sync_duration': sync_duration
            })
            
            print(f"Sincroniza√ß√£o v2 completa: {total_records} registros em {sync_duration:.2f}s")
            
        except Exception as e:
            data_cache['metadata']['sync_status'] = 'error'
            print(f"Erro na sincroniza√ß√£o v2: {e}")

def background_sync_v2():
    """Executa sincroniza√ß√£o otimizada em background"""
    schedule.every(10).minutes.do(sync_all_data_v2)
    
    while True:
        schedule.run_pending()
        time.sleep(30)  # Check mais frequente

# Inicializar pool de conex√µes
create_connection_pool()

# Iniciar thread de sincroniza√ß√£o em background
sync_thread = threading.Thread(target=background_sync_v2, daemon=True)
sync_thread.start()

@app.route('/', methods=['GET'])
def root():
    """Rota raiz do servi√ßo"""
    return jsonify({
        'service': 'fature-real-data-service',
        'version': '2.0',
        'status': 'running',
        'message': 'Fature Real Data Service v2.0 - Operational',
        'endpoints': {
            'health': '/health',
            'users': '/data/v2/users',
            'transactions': '/data/v2/transactions',
            'affiliates': '/data/v2/affiliates',
            'bets': '/data/v2/bets',
            'stats': '/data/v2/stats',
            'sync': '/sync/v2 (POST)'
        },
        'timestamp': datetime.now().isoformat()
    })

@app.route('/health', methods=['GET'])
def health_check_v2():
    """Health check otimizado para Railway - resposta r√°pida garantida"""
    try:
        current_time = datetime.now().isoformat()
        
        # Resposta b√°sica sempre dispon√≠vel
        response_data = {
            'status': 'healthy',
            'service': 'real-data-service-v2',
            'version': '2.0',
            'timestamp': current_time,
            'message': 'Service is running'
        }
        
        # Tentar informa√ß√µes adicionais sem bloquear
        try:
            # Verificar pool de conex√µes rapidamente
            if 'connection_pool' in globals():
                response_data['connection_pool_size'] = len(connection_pool)
                response_data['external_db_status'] = 'pool_available'
            else:
                response_data['external_db_status'] = 'initializing'
            
            # Status do cache
            response_data['sync_status'] = data_cache['metadata']['sync_status']
            
        except Exception as info_error:
            # Se falhar ao obter informa√ß√µes extras, continua com resposta b√°sica
            response_data['external_db_status'] = 'unknown'
            response_data['sync_status'] = 'unknown'
        
        print(f"‚úÖ Health check OK - {current_time}")
        return jsonify(response_data), 200
        
    except Exception as e:
        # Mesmo em caso de erro, retorna resposta r√°pida
        print(f"‚ùå Health check error: {e}")
        return jsonify({
            'status': 'healthy',  # Mant√©m healthy para n√£o falhar o deploy
            'service': 'real-data-service-v2',
            'timestamp': datetime.now().isoformat(),
            'message': 'Service is running with limited info',
            'error': str(e)
        }), 200  # Retorna 200 mesmo com erro para passar no healthcheck

@app.route('/sync/v2', methods=['POST'])
def manual_sync_v2():
    """Sincroniza√ß√£o manual otimizada"""
    threading.Thread(target=sync_all_data_v2, daemon=True).start()
    return jsonify({
        'message': 'Sincroniza√ß√£o v2 iniciada',
        'timestamp': datetime.now().isoformat(),
        'status': data_cache['metadata']['sync_status']
    })

@app.route('/data/v2/users', methods=['GET'])
def get_users_v2():
    """Retorna dados de usu√°rios com busca otimizada"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 50, type=int)
    partition = request.args.get('partition', None, type=int)
    
    with cache_lock:
        if partition is not None and partition in data_cache['users']:
            # Busca em parti√ß√£o espec√≠fica
            users_data = data_cache['users'][partition]
        else:
            # Busca em todas as parti√ß√µes
            users_data = []
            for partition_data in data_cache['users'].values():
                users_data.extend(partition_data)
    
    # Pagina√ß√£o
    start = (page - 1) * per_page
    end = start + per_page
    users = users_data[start:end]
    total = len(users_data)
    
    return jsonify({
        'users': users,
        'pagination': {
            'page': page,
            'per_page': per_page,
            'total': total,
            'pages': (total + per_page - 1) // per_page
        },
        'metadata': {
            'last_sync': data_cache['metadata']['last_sync'].isoformat() if data_cache['metadata']['last_sync'] else None,
            'partitions_available': list(data_cache['users'].keys()),
            'version': '2.0'
        }
    })

@app.route('/data/v2/transactions', methods=['GET'])
def get_transactions_v2():
    """Retorna dados de transa√ß√µes com busca otimizada"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 50, type=int)
    partition = request.args.get('partition', None, type=int)
    
    with cache_lock:
        if partition is not None and partition in data_cache['transactions']:
            transactions_data = data_cache['transactions'][partition]
        else:
            transactions_data = []
            for partition_data in data_cache['transactions'].values():
                transactions_data.extend(partition_data)
    
    start = (page - 1) * per_page
    end = start + per_page
    transactions = transactions_data[start:end]
    total = len(transactions_data)
    
    return jsonify({
        'transactions': transactions,
        'pagination': {
            'page': page,
            'per_page': per_page,
            'total': total,
            'pages': (total + per_page - 1) // per_page
        },
        'metadata': {
            'last_sync': data_cache['metadata']['last_sync'].isoformat() if data_cache['metadata']['last_sync'] else None,
            'partitions_available': list(data_cache['transactions'].keys()),
            'version': '2.0'
        }
    })

@app.route('/data/v2/affiliates', methods=['GET'])
def get_affiliates_v2():
    """Retorna dados de afiliados com busca otimizada"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 50, type=int)
    partition = request.args.get('partition', None, type=int)
    
    with cache_lock:
        if partition is not None and partition in data_cache['affiliates']:
            affiliates_data = data_cache['affiliates'][partition]
        else:
            affiliates_data = []
            for partition_data in data_cache['affiliates'].values():
                affiliates_data.extend(partition_data)
    
    start = (page - 1) * per_page
    end = start + per_page
    affiliates = affiliates_data[start:end]
    total = len(affiliates_data)
    
    return jsonify({
        'affiliates': affiliates,
        'pagination': {
            'page': page,
            'per_page': per_page,
            'total': total,
            'pages': (total + per_page - 1) // per_page
        },
        'metadata': {
            'last_sync': data_cache['metadata']['last_sync'].isoformat() if data_cache['metadata']['last_sync'] else None,
            'partitions_available': list(data_cache['affiliates'].keys()),
            'version': '2.0'
        }
    })

@app.route('/data/v2/bets', methods=['GET'])
def get_bets_v2():
    """Retorna dados de apostas com busca otimizada"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 50, type=int)
    partition = request.args.get('partition', None, type=int)
    
    with cache_lock:
        if partition is not None and partition in data_cache['bets']:
            bets_data = data_cache['bets'][partition]
        else:
            bets_data = []
            for partition_data in data_cache['bets'].values():
                bets_data.extend(partition_data)
    
    start = (page - 1) * per_page
    end = start + per_page
    bets = bets_data[start:end]
    total = len(bets_data)
    
    return jsonify({
        'bets': bets,
        'pagination': {
            'page': page,
            'per_page': per_page,
            'total': total,
            'pages': (total + per_page - 1) // per_page
        },
        'metadata': {
            'last_sync': data_cache['metadata']['last_sync'].isoformat() if data_cache['metadata']['last_sync'] else None,
            'partitions_available': list(data_cache['bets'].keys()),
            'version': '2.0'
        }
    })

@app.route('/data/v2/stats', methods=['GET'])
def get_stats_v2():
    """Estat√≠sticas avan√ßadas dos dados"""
    with cache_lock:
        stats = {
            'version': '2.0',
            'total_records': data_cache['metadata']['total_records'],
            'partitions': data_cache['metadata']['partitions'],
            'last_sync': data_cache['metadata']['last_sync'].isoformat() if data_cache['metadata']['last_sync'] else None,
            'sync_status': data_cache['metadata']['sync_status'],
            'sync_duration': data_cache['metadata']['sync_duration'],
            'records_by_table': {},
            'records_by_partition': {}
        }
        
        # Contar registros por tabela e parti√ß√£o
        for table in ['users', 'transactions', 'affiliates', 'bets']:
            table_total = sum(len(partition) for partition in data_cache[table].values())
            stats['records_by_table'][table] = table_total
            
            partition_counts = {
                str(partition_id): len(partition_data)
                for partition_id, partition_data in data_cache[table].items()
            }
            stats['records_by_partition'][table] = partition_counts
    
    return jsonify(stats)

@app.route('/performance/test', methods=['GET'])
def performance_test():
    """Teste de performance do servi√ßo v2"""
    start_time = time.time()
    
    # Simular v√°rias opera√ß√µes
    operations = []
    
    # Teste de busca em parti√ß√µes
    for partition in range(4):
        partition_start = time.time()
        with cache_lock:
            users_count = len(data_cache['users'].get(partition, []))
            transactions_count = len(data_cache['transactions'].get(partition, []))
        partition_time = time.time() - partition_start
        
        operations.append({
            'operation': f'partition_{partition}_query',
            'duration': partition_time,
            'users_count': users_count,
            'transactions_count': transactions_count
        })
    
    total_time = time.time() - start_time
    
    return jsonify({
        'performance_test': {
            'total_duration': total_time,
            'operations': operations,
            'avg_operation_time': total_time / len(operations) if operations else 0,
            'timestamp': datetime.now().isoformat()
        }
    })

# Executar sincroniza√ß√£o inicial ao iniciar o servi√ßo
def initial_sync_v2():
    """Executa sincroniza√ß√£o inicial otimizada"""
    threading.Thread(target=sync_all_data_v2, daemon=True).start()

if __name__ == '__main__':
    # Logs de inicializa√ß√£o
    print("=" * 50)
    print("üöÄ INICIANDO FATURE REAL DATA SERVICE V2")
    print("=" * 50)
    
    # Verificar vari√°veis de ambiente
    port = int(os.getenv('PORT', 5000))
    print(f"üì° Porta configurada: {port}")
    print(f"üóÑÔ∏è  Banco: {EXTERNAL_DB_CONFIG['host']}:{EXTERNAL_DB_CONFIG['port']}")
    
    # Inicializar componentes em background para n√£o bloquear o startup
    def initialize_background_services():
        """Inicializa servi√ßos em background ap√≥s o Flask estar rodando"""
        time.sleep(2)  # Aguarda Flask inicializar
        
        # Testar conex√£o com banco
        print("üîç Testando conex√£o com banco...")
        try:
            test_conn = psycopg2.connect(**EXTERNAL_DB_CONFIG)
            test_conn.close()
            print("‚úÖ Conex√£o com banco OK!")
        except Exception as e:
            print(f"‚ùå Erro na conex√£o com banco: {e}")
            print("‚ö†Ô∏è  Servi√ßo continuar√° sem sincroniza√ß√£o inicial")
        
        # Inicializar pool de conex√µes
        print("üîß Inicializando pool de conex√µes...")
        create_connection_pool()
        
        # Iniciar scheduler em thread separada
        print("‚è∞ Iniciando scheduler...")
        scheduler_thread = threading.Thread(target=run_scheduler_v2, daemon=True)
        scheduler_thread.start()
        
        # Executar sincroniza√ß√£o inicial (n√£o bloqueante)
        print("üîÑ Iniciando sincroniza√ß√£o inicial...")
        initial_sync_v2()
        
        print("‚úÖ Todos os servi√ßos em background inicializados!")
    
    # Iniciar servi√ßos em background
    background_init_thread = threading.Thread(target=initialize_background_services, daemon=True)
    background_init_thread.start()
    
    print("=" * 50)
    print(f"üåê Servidor iniciando em 0.0.0.0:{port}")
    print("üìã Endpoints dispon√≠veis:")
    print("   - GET /health - Health check")
    print("   - GET /data/v2/users - Dados de usu√°rios")
    print("   - GET /data/v2/stats - Estat√≠sticas")
    print("=" * 50)
    
    # Iniciar aplica√ß√£o Flask imediatamente
    app.run(host='0.0.0.0', port=port, debug=False)

